{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5fa24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39c93caf",
   "metadata": {},
   "source": [
    "## Extract Emotion Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f12b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the encodings that we will use\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encodings=[\"love\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\", \"neutral\", \"other\"]\n",
    "label_encodings.sort()\n",
    "label_encoder.fit(label_encodings)\n",
    "\n",
    "# Print out the emotions to check it has been loaded successfully\n",
    "emotions = label_encoder.classes_\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a23d4e",
   "metadata": {},
   "source": [
    "## Load Classifier and Scaler\n",
    "\n",
    "Load logistic regression classifier and pre-fit scaler and store the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classifier_path to location of the logistic regression classifier\n",
    "classifier_path = '../../Resources/Models/nli-mpnet-base-v2-LR-classifier.pkl'\n",
    "\n",
    "try: \n",
    "    # Load and store model in sentiment_model\n",
    "    file = open(classifier_path, 'rb')\n",
    "    sentiment_model = pickle.load(file)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error while opening file: {e}\")\n",
    "    \n",
    "finally:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c929a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scaler_path to location of the pre-fit scaler\n",
    "scaler_path = '../../Resources/Models/sentimentScaler.pkl'\n",
    "\n",
    "try: \n",
    "    # Load and store model in scaler\n",
    "    file = open(scaler_path, 'rb')\n",
    "    scaler = pickle.load(file)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error while opening file: {e}\")\n",
    "    \n",
    "finally:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bcf4e",
   "metadata": {},
   "source": [
    "## Predict Labels with Multiprocessing\n",
    "\n",
    "By using SBERT model and the array of parsed sentences, we will now output a corresponding array where each element is a tuple of the predicted label and an array of the raw possibilities for each label. The emotions outputted should match that of the previous cell.\n",
    "\n",
    "This process uses multiprocessing to efficiently run large-sized corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sbert_path to location of SBERT model\n",
    "sbert_path = '../../Resources/Models/nli-mpnet-base-v2'\n",
    "transformer = ST(sbert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e7dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding through multiprocessing\n",
    "def encode_sentence(sent):\n",
    "    \n",
    "    # Encode chunk of sentences in parsed_sents array\n",
    "    sentence_embedding = transformer.encode(sent, show_progress_bar=False)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using multiple processes, important to eventually close them to avoid memory/resource leaks\n",
    "try:\n",
    "    start = perf_counter()\n",
    "\n",
    "    # Define a thread Pool to process multiple sentences simultaneously\n",
    "    # Default set to num_cores, but may change number of processes depending on instance\n",
    "    cores_used = num_cores - 1\n",
    "    p_encode = Pool(processes=cores_used)\n",
    "    \n",
    "    # Apply function with Pool to array\n",
    "    chunksize = int(len(parsed_sents) / cores_used)\n",
    "    sentence_embeddings = p_encode.map(encode_sentence, parsed_sents, chunksize)\n",
    "    \n",
    "    end = perf_counter()\n",
    "    \n",
    "    total_minutes = (end - start) / 60\n",
    "    total_seconds = (end - start) % 60\n",
    "\n",
    "    print(f\"Took {int(total_minutes)}min {total_seconds:.2f}s to encode {len(parsed_sents)} sentences.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while encoding sentences: {e}\")\n",
    "\n",
    "finally:\n",
    "    p_encode.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both scaler and sentiment_model should exist before running this cell\n",
    "if scaler is not None and sentiment_model is not None: \n",
    "    standardized = scaler.transform(sentence_embeddings)\n",
    "        \n",
    "    y_pred_numeric = sentiment_model.predict(standardized)\n",
    "    y_pred_string = label_encoder.inverse_transform(y_pred_numeric)\n",
    "        \n",
    "    # Call the predict function on our sentences\n",
    "    raw_predictions = sentiment_model.predict_proba(standardized)\n",
    "        \n",
    "    results = list(zip(y_pred_string, raw_predictions))\n",
    "    \n",
    "    # Print first element of array\n",
    "    print(results[0:5])\n",
    "else: \n",
    "    print(\"Please load scaler and sentiment model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87705d8",
   "metadata": {},
   "source": [
    "## Create Dataframe\n",
    "\n",
    "Create a dataframe to show the predicted labels and each estimated value for each emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make column list\n",
    "columns = ['GOID', 'Date', 'Pub ID', 'Sentence', 'Label']\n",
    "columns.extend(emotions)\n",
    "\n",
    "# Create dictionary for dataframe\n",
    "data = {}\n",
    "\n",
    "for col in columns: \n",
    "    data[col] = []\n",
    "\n",
    "# Fill out each row using the collected data\n",
    "for index,result in enumerate(results):\n",
    "    data['GOID'].append(parsed_goids[index])\n",
    "    data['Date'].append(parsed_dates[index])\n",
    "    data['Pub ID'].append(parsed_pubids[index])\n",
    "    data['Sentence'].append(parsed_sents[index])\n",
    "    data['Label'].append(result[0])\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        data[emotion].append(result[1][i])\n",
    "\n",
    "results_df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View final dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51659036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-level dataframe\n",
    "means_df = results_df.groupby(['GOID'], as_index=True).agg({'Date': 'first',\n",
    "                                                            'anger':'mean',\n",
    "                                                            'disgust':'mean',\n",
    "                                                            'fear':'mean',\n",
    "                                                            'happiness':'mean',\n",
    "                                                            'love':'mean',\n",
    "                                                            'neutral':'mean',\n",
    "                                                            'other':'mean',\n",
    "                                                            'sadness':'mean',\n",
    "                                                            'surprise':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View document-level dataframe\n",
    "means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c120b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to file\n",
    "means_df.to_csv(means_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to file\n",
    "results_df.to_csv(results_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
